# Workflow Automation Guide for AI Website Builder Testing

## Overview
This guide helps you automate and streamline the process of testing 15-20 AI website builders with consistent brand/design prompts.

## Repository Structure

```
janet-jeffus-portfolio/
├── README.md
├── .github/
│   └── workflows/
│       └── deploy.yml
├── content/
│   ├── en/
│   │   ├── about.md
│   │   ├── skills.md
│   │   ├── projects.md
│   │   └── contact.md
│   └── translations/
│       └── [generated by build]
├── prompts/
│   ├── brand_prompt.md
│   ├── website_prompt.md
│   └── simplified_prompt.md
├── testing/
│   ├── testing_tracker.md
│   ├── screenshots/
│   │   ├── tool1/
│   │   ├── tool2/
│   │   └── ...
│   └── results/
│       ├── tool1_report.md
│       ├── tool2_report.md
│       └── ...
├── generated-sites/
│   ├── v0/
│   ├── lovable/
│   ├── builder-io/
│   └── [one folder per tool]
├── hub-site/
│   ├── index.html
│   ├── styles.css
│   └── assets/
└── scripts/
    ├── translate.js
    ├── build.js
    └── deploy.sh
```

## Step-by-Step Workflow

### Phase 1: Preparation (30 minutes)

1. **Create Main GitHub Repository**
   ```bash
   mkdir janet-jeffus-portfolio
   cd janet-jeffus-portfolio
   git init
   git remote add origin [your-repo-url]
   ```

2. **Set Up Directory Structure**
   ```bash
   mkdir -p .github/workflows content/{en,translations} prompts testing/{screenshots,results} generated-sites hub-site scripts
   ```

3. **Copy Prompt Files**
   - Place `brand_prompt.md` in `prompts/`
   - Place `website_prompt.md` in `prompts/`
   - Place `simplified_prompt.md` in `prompts/`
   - Place `testing_tracker.md` in `testing/`

4. **Create Content Files**
   - Write your `about.md`, `skills.md`, `projects.md`, `contact.md` in `content/en/`
   - These will be your source of truth for all sites

### Phase 2: Testing Tools (5-6 hours total, ~20 min per tool)

For each AI tool, follow this process:

#### A. Pre-Test Setup (2 minutes)
1. Open `testing/testing_tracker.md`
2. Create a new entry for this tool
3. Note start time

#### B. Tool Testing (15 minutes)
1. **Sign up/Login** to the AI tool
2. **Input Prompts:**
   - Copy full text from `brand_prompt.md`
   - Copy relevant sections from `website_prompt.md` or use `simplified_prompt.md` for opinionated tools
3. **Generate Initial Site**
   - Note how long it takes
   - Take screenshot of initial result
4. **Iterate 2-3 Times:**
   - Make refinements based on output
   - Try to match the terminal aesthetic
   - Test mobile responsiveness
5. **Test Features:**
   - Keyboard navigation (1-9 keys)
   - Mobile view
   - Performance (Lighthouse)
   - Accessibility

#### C. Documentation (3 minutes)
1. **Take Screenshots:**
   ```bash
   # Save to testing/screenshots/[tool-name]/
   - desktop-home.png
   - mobile-home.png
   - section-skills.png
   - section-about.png
   - lighthouse-report.png
   ```

2. **Export/Save Code:**
   - Export source code if possible
   - Save to `generated-sites/[tool-name]/`
   - Create README.md with setup instructions

3. **Fill Out Testing Notes:**
   - Complete the template in `testing_tracker.md`
   - Add scores for each criterion
   - Document pros/cons

4. **Create GitHub Repository (if code is accessible):**
   ```bash
   cd generated-sites/[tool-name]
   git init
   git add .
   git commit -m "Initial site from [tool-name]"
   git remote add origin [tool-specific-repo-url]
   git push -u origin main
   ```

#### D. Deployment Testing (5 minutes)
1. **Attempt Cloudflare Pages Deployment:**
   - If tool allows export, deploy manually
   - If tool has native hosting, note deployment URL
   - Document ease of deployment

2. **Record URLs:**
   - Live site URL
   - GitHub repository URL (if applicable)
   - Add to tracking sheet

### Phase 3: Hub Site Creation (2-3 hours)

After testing all tools, create the hub site:

#### A. Design Hub Site (1 hour)
Create `hub-site/index.html` with:
- Terminal-style interface (use your brand aesthetic!)
- Grid/list of all tested tools
- Preview thumbnails for each
- Key metrics displayed
- Links to live sites and repos
- Filtering/sorting options

#### B. Implement Comparison Features (1 hour)
```html
<!-- Example structure -->
<div class="tool-card">
  <h3>Tool Name</h3>
  <img src="screenshots/tool/preview.png" alt="Preview">
  <div class="metrics">
    <span>Design: ⭐⭐⭐⭐</span>
    <span>Performance: ⭐⭐⭐⭐⭐</span>
    <span>Code Quality: ⭐⭐⭐</span>
  </div>
  <div class="links">
    <a href="[live-site]">View Live</a>
    <a href="[github-repo]">Source Code</a>
  </div>
</div>
```

#### C. Add Metadata & Documentation (30 minutes)
- Write methodology explanation
- Include the prompts used
- Add comparison table
- Create filtering interface

#### D. Deploy Hub Site (30 minutes)
```bash
cd hub-site
# Deploy to Cloudflare Pages
wrangler pages project create janet-ai-comparison
wrangler pages publish . --project-name=janet-ai-comparison
```

### Phase 4: Documentation & Analysis (2-3 hours)

#### A. Write Comprehensive Report (1-2 hours)
Create `testing/comparison_report.md`:
1. **Executive Summary**
   - Top 3 tools
   - Key findings
   - Recommendations

2. **Methodology**
   - Testing approach
   - Criteria used
   - Limitations

3. **Individual Tool Reviews**
   - One section per tool
   - Scores and analysis
   - Screenshots

4. **Comparative Analysis**
   - Best for terminal aesthetic
   - Best for code quality
   - Best for ease of use
   - Best value for money
   - Best overall

5. **Conclusion & Recommendations**

#### B. Create Presentation/Blog Post (1 hour)
Options:
- Medium article
- Dev.to post
- LinkedIn article
- Conference talk slides
- YouTube video script

### Phase 5: Automation (Optional, 3-4 hours)

#### A. Translation Automation
Create `scripts/translate.js`:
```javascript
// Pseudo-code for translation automation
const fs = require('fs');
const Anthropic = require('@anthropic-ai/sdk');

async function translateFile(filePath, targetLang) {
  const content = fs.readFileSync(filePath, 'utf8');
  const hash = getHash(content);
  
  // Check if translation exists and is up-to-date
  if (translationCache[hash]?.[targetLang]) {
    return translationCache[hash][targetLang];
  }
  
  // Translate with AI
  const anthropic = new Anthropic({
    apiKey: process.env.ANTHROPIC_API_KEY,
  });
  
  const message = await anthropic.messages.create({
    model: "claude-sonnet-4-20250514",
    max_tokens: 4000,
    messages: [{
      role: "user",
      content: `Translate this markdown content to ${targetLang}. Preserve all markdown formatting, links, and code blocks:\n\n${content}`
    }]
  });
  
  const translated = message.content[0].text;
  
  // Cache the translation
  saveToCache(hash, targetLang, translated);
  
  return translated;
}
```

#### B. Build Automation
Create `.github/workflows/deploy.yml`:
```yaml
name: Deploy to Cloudflare Pages

on:
  push:
    branches: [ main ]
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
      
      - name: Install dependencies
        run: npm install
      
      - name: Translate content
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: node scripts/translate.js
      
      - name: Build site
        run: npm run build
      
      - name: Deploy to Cloudflare Pages
        uses: cloudflare/pages-action@v1
        with:
          apiToken: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          accountId: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          projectName: janet-jeffus-portfolio
          directory: dist
```

#### C. Screenshot Automation (Optional)
Create `scripts/screenshot.js` using Puppeteer:
```javascript
const puppeteer = require('puppeteer');

async function captureScreenshots(url, toolName) {
  const browser = await puppeteer.launch();
  const page = await browser.newPage();
  
  // Desktop
  await page.setViewport({ width: 1920, height: 1080 });
  await page.goto(url);
  await page.screenshot({ 
    path: `testing/screenshots/${toolName}/desktop.png`,
    fullPage: true 
  });
  
  // Mobile
  await page.setViewport({ width: 375, height: 812 });
  await page.screenshot({ 
    path: `testing/screenshots/${toolName}/mobile.png`,
    fullPage: true 
  });
  
  await browser.close();
}
```

## Time Estimates

| Phase | Estimated Time |
|-------|---------------|
| Preparation | 30 minutes |
| Testing 15 tools (20 min each) | 5 hours |
| Hub site creation | 2-3 hours |
| Documentation & analysis | 2-3 hours |
| Automation (optional) | 3-4 hours |
| **Total (without automation)** | **~10 hours** |
| **Total (with automation)** | **~13-14 hours** |

## Tips for Efficiency

### 1. Batch Similar Tasks
- Do all signups at once
- Test all mobile views together
- Run all Lighthouse audits in sequence

### 2. Use Templates
- Create reusable prompt templates
- Pre-fill testing notes template
- Standardize file naming

### 3. Keyboard Shortcuts
- Learn your screenshot tool shortcuts
- Use browser DevTools efficiently
- Master git commands for quick commits

### 4. Parallel Testing
- Open multiple tabs for different tools
- Let one site build while testing another
- Run Lighthouse audits while documenting

### 5. Take Notes As You Go
- Don't wait until the end to document
- Voice notes for quick thoughts
- Screenshots immediately when you see something notable

## Quality Checklist

Before considering a tool "done":
- [ ] Screenshots captured (desktop + mobile)
- [ ] Source code saved (if accessible)
- [ ] Testing notes completed
- [ ] Scores recorded in tracker
- [ ] GitHub repo created (if applicable)
- [ ] Live site URL recorded
- [ ] Lighthouse audit run
- [ ] Mobile responsiveness tested
- [ ] Keyboard navigation tested (1-9 keys)
- [ ] Accessibility spot-checked

## Common Issues & Solutions

### Issue: Tool doesn't understand terminal aesthetic
**Solution:** 
- Break prompt into smaller parts
- Provide specific examples
- Use reference images (your terminal screenshots)
- Iterate with more specific instructions

### Issue: Can't export code
**Solution:**
- Take detailed screenshots
- Document approach and limitations
- Note in testing tracker
- Focus on other evaluation criteria

### Issue: Tool is too expensive to test thoroughly
**Solution:**
- Use free trial fully
- Document trial limitations
- Consider if the cost is worth it
- Test only core features

### Issue: Output is way off-brand
**Solution:**
- Try 2-3 more iterations
- Adjust prompt strategy
- If still off, document and move on
- Some tools just won't work for this use case

## Final Deliverable Checklist

When project is complete:
- [ ] Main repository with all prompts
- [ ] Testing tracker fully completed
- [ ] 15-20 individual site repos (where possible)
- [ ] Hub site deployed and live
- [ ] Comprehensive comparison report
- [ ] Screenshot collection organized
- [ ] Blog post or presentation (optional)
- [ ] Social media promotion content (optional)

## Maintenance Plan

After initial testing:
- Update tracker as tools evolve
- Re-test top 3 tools quarterly
- Add new AI builders as they emerge
- Keep hub site updated with latest URLs
- Maintain working demos

---

## Quick Start Commands

```bash
# Clone starter template
git clone [your-main-repo] janet-portfolio-testing
cd janet-portfolio-testing

# Install dependencies (if using automation)
npm install

# Start testing
# (Follow Phase 2 workflow for each tool)

# Build hub site
cd hub-site
# [Edit files]

# Deploy hub to Cloudflare Pages
wrangler pages publish . --project-name=janet-ai-comparison

# Generate final report
# (Compile all testing notes into comparison_report.md)
```
